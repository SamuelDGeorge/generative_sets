{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import reset_graph\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tan.tan_util import get_tan_nll as tan\n",
    "from tan.tan_util import get_tan_nll_cond as tan_cond\n",
    "\n",
    "\n",
    "#For parsing records once written\n",
    "from Utilities.set_record_parser import build_set_dataset\n",
    "from Utilities.set_record_parser import get_file_lists\n",
    "from Utilities.models import log_dir_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Directories \n",
    "\n",
    "Here we are going to get the files needed to do the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate Neccesary Files\n",
    "#Get Imageanet Label and Record Location\n",
    "labels_file = \"D:/Machine_Learning/Datasets/ImageNet_2012/labels.txt\"\n",
    "tf_record_directory =  'D:/Machine_Learning/Datasets/Set_Project/Code_Class_TF_Records'\n",
    "\n",
    "#Make \n",
    "class_file = open(labels_file,'r')\n",
    "labels = class_file.read().split('\\n')\n",
    "class_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Machine_Learning/Datasets/Set_Project/Code_Class_TF_Records\\\\Training_Set-00000-of-00002',\n",
       " 'D:/Machine_Learning/Datasets/Set_Project/Code_Class_TF_Records\\\\Training_Set-00001-of-00002']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import location of TF_Records\n",
    "train_list, val_list = get_file_lists(tf_record_directory)\n",
    "train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network\n",
    "\n",
    "The following provides the code to import and use the TF_Records for the set project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for logs in training\n",
    "set_net_logs = 'D:/AI/models/set_project/logs'\n",
    "model_path = log_dir_build(set_net_logs, \"set_project\")\n",
    "\n",
    "#directory for all the models saved during training\n",
    "set_net_model = 'D:/AI/models/set_project/model/' + 'set_project'\n",
    "set_net_model_best = 'D:/AI/models/set_project/model/' + 'set_project_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_set_layer(input_code, input_size, condense_size, set_size=3, layer_name='default', activation_func=tf.nn.sigmoid):\n",
    "    \n",
    "    learned_transform = tf.get_variable(layer_name + '_transform', shape=[input_size,condense_size], \n",
    "                                        trainable=True, initializer=tf.contrib.layers.variance_scaling_initializer()) \n",
    "    batched_transform = tf.broadcast_to(learned_transform, [tf.shape(input_code)[0], input_size, condense_size])\n",
    "    transform_layer = tf.matmul(input_code, batched_transform)\n",
    "    activation = activation_func(transform_layer)\n",
    "    \n",
    "    \n",
    "    lambda_1 = tf.get_variable(layer_name + \"_lambda\", [condense_size], trainable=True, dtype=tf.float32, initializer=tf.initializers.random_normal(mean=1))\n",
    "    lambda_1_transformed = tf.broadcast_to(lambda_1, [set_size, condense_size])\n",
    "    multipy_pairwise = tf.broadcast_to(lambda_1_transformed, [tf.shape(input_code)[0], set_size, condense_size])\n",
    "    \n",
    "    sigma_1 = tf.abs(tf.get_variable(layer_name + \"_sigma\", [condense_size], trainable=True, dtype=tf.float32, initializer=tf.initializers.random_normal(mean=0)))\n",
    "    sigma_1_tranformed = tf.broadcast_to(sigma_1, [tf.shape(input_code)[0], condense_size])\n",
    "    \n",
    "\n",
    "    # + sigma * mean(Data)\n",
    "    max_pool_1 = tf.reduce_mean(activation, axis=1)\n",
    "    sum_term = tf.multiply(sigma_1_tranformed, max_pool_1)\n",
    "    sum_term_final = tf.expand_dims(sum_term, axis=1)  \n",
    "    \n",
    "    pre_activation_1 = tf.multiply(activation, multipy_pairwise) + sum_term_final\n",
    "    layer_1 = activation_func(pre_activation_1)\n",
    "    return layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "#Placeholder for choosing input, epochs, batches, and datasets at runtime\n",
    "number_of_classes = 100\n",
    "learning_rate_class = .1\n",
    "dropout_rate = 0.2\n",
    "set_size = 3\n",
    "\n",
    "with tf.name_scope('Data_Retrieval'):\n",
    "    filename = tf.placeholder(tf.string, shape=[None], name=\"tf_records\")\n",
    "    batch_size = tf.placeholder(tf.int64, shape=[], name= \"Batch_Size\")\n",
    "    num_epochs = tf.placeholder(tf.int64, shape=[], name= \"Num_epochs\")\n",
    "    training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "    handle = tf.placeholder(tf.string, shape=[], name=\"Dataset\")\n",
    "    code_size = 2048\n",
    "\n",
    "    training_set = build_set_dataset(True, filename, code_size, set_size, batch_size, num_epochs, num_parallel_calls=8)\n",
    "    validation_set = build_set_dataset(False, filename, code_size, set_size, batch_size, num_epochs, num_parallel_calls=8)\n",
    "\n",
    "    train_iterator = training_set.make_initializable_iterator()\n",
    "    val_iterator = validation_set.make_initializable_iterator()\n",
    "\n",
    "    iterator = tf.data.Iterator.from_string_handle(\n",
    "        handle, training_set.output_types, training_set.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "    code_data, class_data, file_data, uniques_data = next_element\n",
    "\n",
    "    code = tf.placeholder_with_default(code_data, [None,3,2048])\n",
    "    uniques = tf.placeholder_with_default(uniques_data, [None])\n",
    "\n",
    "with tf.name_scope(\"BN_Layer_AE_Layers\"):\n",
    "    #Define initalizer and batch normalization layers\n",
    "    bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"Set_Analyzer\"):\n",
    "    #the network for generating output of our set\n",
    "    code_size = 2048\n",
    "    n_layer_1 = 1000\n",
    "    n_layer_2 = 500\n",
    "    n_layer_3 = 250\n",
    "    n_final_layer = 100\n",
    "    deep_activation = tf.nn.relu\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('Deep_Sets'):\n",
    "        batch_item = tf.cast(batch_size, tf.int32)\n",
    "        input_before = tf.cast(code, tf.float32)\n",
    "        \n",
    "        with tf.name_scope('DeepSet_Layer_1'):\n",
    "            #1000\n",
    "            deep_1 = deep_set_layer(input_before, code_size, n_layer_1, set_size=3, layer_name='Deep_One', activation_func=deep_activation)\n",
    "\n",
    "        with tf.name_scope('DeepSet_Layer_2'):\n",
    "            #500\n",
    "            deep_2 = deep_set_layer(deep_1, n_layer_1, n_layer_2, set_size=3, layer_name='Deep_Two', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('DeepSet_Layer_3'):\n",
    "            #250\n",
    "            deep_3 = deep_set_layer(deep_2, n_layer_2, n_layer_3, set_size=3, layer_name='Deep_Three', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('Final_Deep_Pool'):\n",
    "            #250\n",
    "            final_deep_layer = tf.reduce_sum(deep_3, 1)\n",
    "            \n",
    "            \n",
    "    with tf.name_scope('Dense_Layers_Classification'):      \n",
    "        \n",
    "        with tf.name_scope(\"Class_Hidden_Layer_1\"):\n",
    "            #100\n",
    "            hidden1_cat = tf.layers.dense(final_deep_layer, n_final_layer, name=\"hidden1_cat\", kernel_initializer=he_init)\n",
    "            hidden1_drop = tf.layers.dropout(hidden1_cat, dropout_rate, training=training)\n",
    "            hidden1_cast = tf.cast(hidden1_drop, tf.float32)\n",
    "            bn1_cat = bn_batch_norm_layer(hidden1_cast)\n",
    "            bn1_act_cat = tf.nn.relu(bn1_cat)  \n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer\"): \n",
    "            #100\n",
    "            logits_before_bn = tf.layers.dense(bn1_act_cat, n_final_layer, name=\"outputs\")\n",
    "            logits = bn_batch_norm_layer(logits_before_bn, name=\"logits\")\n",
    "            softmax = tf.nn.softmax(logits, name=\"final_soft_max\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "                cast_class = tf.cast(class_data, tf.int32)\n",
    "                one_hot = tf.one_hot(cast_class, number_of_classes)\n",
    "                summed_hot = tf.reduce_sum(one_hot, axis=1)\n",
    "                number_in_set = tf.constant([set_size])\n",
    "                divide = tf.cast(tf.tile(number_in_set, [n_final_layer]), tf.float32)\n",
    "                correct_class = tf.divide(summed_hot, divide)\n",
    "                \n",
    "                loss_cat = tf.losses.softmax_cross_entropy(correct_class,softmax)\n",
    "                loss_summary_cat = tf.summary.scalar('loss_summary_cat', loss_cat)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_class)\n",
    "\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            deep_train = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                with tf.control_dependencies(deep_train):\n",
    "                    training_op_class = optimizer.minimize(loss_cat, global_step=global_step)\n",
    "                \n",
    "    with tf.name_scope('Unique_Identify'):\n",
    "        n_unq_1 = 100\n",
    "        n_unq_2 = 10\n",
    "        n_unq_final = 2\n",
    "        \n",
    "        with tf.name_scope('DeepSet_Unq_Layer_1'):\n",
    "            #500 Output\n",
    "            deep_unq_1 = deep_set_layer(deep_1, n_layer_1, n_layer_2, set_size=3, layer_name='Deep_Unq_One', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('DeepSet_Unq_Layer_2'):\n",
    "            #250 Output\n",
    "            deep_unq_2 = deep_set_layer(deep_unq_1, n_layer_2, n_layer_3, set_size=3, layer_name='Deep_Unq_Two', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('Final_Unq_Deep_Pool'):\n",
    "            #250 Output\n",
    "            final_unq_deep_layer = tf.reduce_sum(deep_unq_2, 1)\n",
    "            \n",
    "        with tf.name_scope(\"Unq_Hidden_Layer_1\"):\n",
    "            #100 Output\n",
    "            hidden1_unq = tf.layers.dense(final_unq_deep_layer, n_unq_1, name=\"hidden1_unq\", kernel_initializer=he_init)\n",
    "            hidden1_drop_unq = tf.layers.dropout(hidden1_unq, dropout_rate, training=training)\n",
    "            hidden1_cast_unq = tf.cast(hidden1_drop_unq, tf.float32)\n",
    "            bn1_cat_unq = bn_batch_norm_layer(hidden1_cast_unq)\n",
    "            bn1_act_cat_unq = tf.nn.relu(bn1_cat_unq)  \n",
    "            \n",
    "        with tf.name_scope(\"Unq_Hidden_Layer_2\"):\n",
    "            #10 Output\n",
    "            hidden2_unq = tf.layers.dense(bn1_act_cat_unq, n_unq_2, name=\"hidden2_unq\", kernel_initializer=he_init)\n",
    "            hidden2_drop_unq = tf.layers.dropout(hidden2_unq, dropout_rate, training=training)\n",
    "            bn2_cat_unq = bn_batch_norm_layer(hidden2_drop_unq)\n",
    "            bn2_act_cat_unq = tf.nn.relu(bn2_cat_unq)  \n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer_unq\"): \n",
    "            #Get softmax\n",
    "            logits_before_bn_unq = tf.layers.dense(bn1_act_cat, n_unq_final, name=\"outputs_unq\")\n",
    "            logits_unq = bn_batch_norm_layer(logits_before_bn_unq, name=\"logits_unq\")\n",
    "            softmax_unq = tf.nn.softmax(logits_unq, name=\"final_soft_max_unq\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"Unique_loss\"):\n",
    "            #0 class is different, 1 is the same\n",
    "            select = tf.not_equal(uniques, tf.constant([1], dtype=tf.int32))\n",
    "            zeros = tf.zeros_like(uniques, dtype=tf.int32)\n",
    "            ones = tf.ones_like(uniques, dtype=tf.int32)\n",
    "            labels = tf.squeeze(tf.where(select, zeros, ones))\n",
    "            \n",
    "            \n",
    "            #Get cross entropy from labels\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits_unq)\n",
    "            loss_unq = tf.reduce_mean(xentropy, name=\"loss_unq\")\n",
    "            loss_summary_unq = tf.summary.scalar('loss_summary_unq', loss_unq)\n",
    "            \n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits_unq, labels, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "                \n",
    "        with tf.name_scope(\"unique_train\"):\n",
    "            global_step_unique = tf.Variable(0, trainable=False, name='global_step_unique')\n",
    "            optimizer_unq = tf.train.AdamOptimizer(learning_rate=learning_rate_class)\n",
    "\n",
    "            extra_update_ops_unq = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops_unq):\n",
    "                with tf.control_dependencies(deep_train):\n",
    "                    training_op_unq = optimizer_unq.minimize(loss_unq, global_step=global_step_unique)\n",
    "            \n",
    "\n",
    "init = tf.global_variables_initializer()    \n",
    "saver_total = tf.train.Saver(name=\"Full_Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Graph to log directory\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the network\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    saver_total.save(sess, set_net_model_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "epochs = 1\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    \n",
    "    presum, sm  = sess.run([summed_hot, correct_class], feed_dict={handle: train_handle})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Train the network to both generate conditioning for the network and also classify the type of set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/set_project/model/set_project\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/set_project/logs/set_project-run-20190222205733/\n",
      "Epoch: 1 Class Loss: 4.5992713 Unique Loss: 0.11038895\n",
      "Epoch: 2 Class Loss: 4.582566 Unique Loss: 0.07852856\n",
      "Epoch: 3 Class Loss: 4.556191 Unique Loss: 0.027449317\n",
      "Epoch: 4 Class Loss: 4.5576434 Unique Loss: 0.060965396\n",
      "Epoch: 5 Class Loss: 4.547373 Unique Loss: 0.001974252\n",
      "Epoch: 6 Class Loss: 4.5495086 Unique Loss: 0.058952674\n",
      "Epoch: 7 Class Loss: 4.53889 Unique Loss: 0.072239086\n",
      "Epoch: 8 Class Loss: 4.5417867 Unique Loss: 0.12007271\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "steps_between_test_save = 1\n",
    "batch = 30\n",
    "train_size = 64000\n",
    "#all_data_steps = np.int(np.floor(train_size/batch))\n",
    "all_data_steps = 200\n",
    "lowest_loss = 10000\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_total.restore(sess, set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 1\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #initialize iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: batch, num_epochs:epochs})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: batch, num_epochs:epochs})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    \n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "\n",
    "            #run Training Op\n",
    "            sess.run([training_op_class, training_op_unq], feed_dict={handle: train_handle, batch_size: batch})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0) :\n",
    "            loss_sum, loss_val, loss_un, loss_un_val, acc_sum = sess.run([loss_summary_cat, loss_cat, loss_summary_unq, loss_unq, accuracy_summary], \n",
    "                                                   feed_dict = {handle: val_handle ,training: False, batch_size: batch})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            filewriter.add_summary(loss_un, step)\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Class Loss: \" + str(loss_val) + \" Unique Loss: \" + str(loss_un_val))\n",
    "            if lowest_loss > loss_val:\n",
    "                saver_total.save(sess, set_net_model_best)\n",
    "                lowest_loss = loss_val\n",
    "            saver_total.save(sess, set_net_model)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Training\n",
    "\n",
    "Here we will see how the network performs after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Demonstration\n",
    "\n",
    "Here we are going to show running the bones of the network and look a the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Initialize Train and validation iterator\n",
    "    sess.run(train_iterator.initializer, feed_dict={filename: train_list, batch_size: 1, num_epochs:1})\n",
    "    sess.run(val_iterator.initializer, feed_dict={filename: val_list, batch_size: 1, num_epochs:1})\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "\n",
    "    test_code, test_class, test_file, test_unique = sess.run([code, class_data, file_data, uniques], feed_dict={handle: val_handle,training: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show codes from single example\n",
    "for i in range(0,3):\n",
    "    X_test = test_code[0][i]\n",
    "    x_val = X_test\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.plot(x_val)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show class for each item in set\n",
    "for i in range(0,3):\n",
    "    X_test = test_class[0][i]\n",
    "    print('Class: ' + str(X_test) + ' Label: ' + labels[X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show what images the codes came from\n",
    "print(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show how many unitque Items are in the set\n",
    "test_unique[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
