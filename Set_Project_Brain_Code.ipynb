{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import reset_graph\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tan.tan_util import get_tan_nll as tan\n",
    "from tan.tan_util import get_tan_nll_cond as tan_cond\n",
    "\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import imagenet_helper_files.vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "#For parsing records once written\n",
    "from Utilities.set_record_parser import build_set_dataset\n",
    "from Utilities.set_record_parser import get_file_lists\n",
    "from Utilities.models import log_dir_build\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Directories \n",
    "\n",
    "Here we are going to get the files needed to do the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate Neccesary Files\n",
    "homogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Original_Codes_Train.pkl\", \"rb\")\n",
    "nonhomogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Inhomogeneous_Codes_Train.pkl\", \"rb\")\n",
    "homogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Original_Codes_Test.pkl\", \"rb\")\n",
    "nonhomogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Inhomogeneous_Codes_Test.pkl\", \"rb\")\n",
    "train_homog_l = pickle.load(homogenous_train)\n",
    "train_homog_labels_l = np.zeros(train_homog_l.shape[0])\n",
    "train_nonhomog_l = pickle.load(nonhomogenous_train)\n",
    "train_nonhomog_labels_l = np.ones(train_nonhomog_l.shape[0])\n",
    "test_homog_l = pickle.load(homogenous_test)\n",
    "test_homog_labels_l = np.zeros(test_homog_l.shape[0])\n",
    "test_nonhomog_l = pickle.load(nonhomogenous_test)\n",
    "test_nonhomog_labels_l = np.ones(test_nonhomog_l.shape[0])\n",
    "homogenous_train.close()\n",
    "nonhomogenous_train.close()\n",
    "homogenous_test.close()\n",
    "nonhomogenous_test.close()\n",
    "#Locate Neccesary Files\n",
    "homogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Original_Codes_Train.pkl\", \"rb\")\n",
    "nonhomogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Inhomogeneous_Codes_Train.pkl\", \"rb\")\n",
    "homogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Original_Codes_Test.pkl\", \"rb\")\n",
    "nonhomogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Inhomogeneous_Codes_Test.pkl\", \"rb\")\n",
    "train_homog_r = pickle.load(homogenous_train)\n",
    "train_homog_labels_r = np.zeros(train_homog_r.shape[0])\n",
    "train_nonhomog_r = pickle.load(nonhomogenous_train)\n",
    "train_nonhomog_labels_r = np.ones(train_nonhomog_r.shape[0])\n",
    "test_homog_r = pickle.load(homogenous_test)\n",
    "test_homog_labels_r = np.zeros(test_homog_r.shape[0])\n",
    "test_nonhomog_r = pickle.load(nonhomogenous_test)\n",
    "test_nonhomog_labels_r = np.ones(test_nonhomog_r.shape[0])\n",
    "homogenous_train.close()\n",
    "nonhomogenous_train.close()\n",
    "homogenous_test.close()\n",
    "nonhomogenous_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine into full train and test set as well as ground truth\n",
    "full_train = np.concatenate((train_homog_l, train_nonhomog_l, train_homog_r, train_nonhomog_r), axis = 0)\n",
    "full_train_labels = np.concatenate((train_homog_labels_l, train_nonhomog_labels_l, train_homog_labels_r, train_nonhomog_labels_r), axis=0)\n",
    "\n",
    "full_test = np.concatenate((test_homog_l, test_nonhomog_l, test_homog_r, test_nonhomog_r), axis = 0)\n",
    "full_test_labels = np.concatenate((test_homog_labels_l, test_nonhomog_labels_l, test_homog_labels_r, test_nonhomog_labels_r), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 3, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_1 = full_test[0]\n",
    "full_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network\n",
    "\n",
    "The following provides the code to import and use the TF_Records for the set project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for logs in training\n",
    "set_net_logs = 'D:/AI/models/set_project_brain_code/logs'\n",
    "model_path = log_dir_build(set_net_logs, \"set_project_brain_code\")\n",
    "#model_path = 'D:/AI/models/set_project_brain/logs/set_project-run-20190306174848/'\n",
    "\n",
    "#directory for all the models saved during training\n",
    "set_net_model = 'D:/AI/models/set_project_brain_code/model/' + 'set_project'\n",
    "set_net_model_best = 'D:/AI/models/set_project_brain_code/model/' + 'set_project_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_set_layer(input_code, input_size, condense_size, set_size=3, layer_name='default', activation_func=tf.nn.sigmoid):\n",
    "    \n",
    "    learned_transform = tf.get_variable(layer_name + '_transform', shape=[input_size,condense_size], \n",
    "                                        trainable=True, initializer=tf.contrib.layers.variance_scaling_initializer()) \n",
    "    batched_transform = tf.broadcast_to(learned_transform, [tf.shape(input_code)[0], input_size, condense_size])\n",
    "    transform_layer = tf.matmul(input_code, batched_transform)\n",
    "    activation = activation_func(transform_layer)\n",
    "    \n",
    "    \n",
    "    lambda_1 = tf.get_variable(layer_name + \"_lambda\", [condense_size], trainable=True, dtype=tf.float32, initializer=tf.initializers.random_normal(mean=1))\n",
    "    lambda_1_transformed = tf.broadcast_to(lambda_1, [set_size, condense_size])\n",
    "    multipy_pairwise = tf.broadcast_to(lambda_1_transformed, [tf.shape(input_code)[0], set_size, condense_size])\n",
    "    \n",
    "    sigma_1 = tf.abs(tf.get_variable(layer_name + \"_sigma\", [condense_size], trainable=True, dtype=tf.float32, initializer=tf.initializers.random_normal(mean=0)))\n",
    "    sigma_1_tranformed = tf.broadcast_to(sigma_1, [tf.shape(input_code)[0], condense_size])\n",
    "    \n",
    "\n",
    "    # + sigma * mean(Data)\n",
    "    max_pool_1 = tf.reduce_mean(activation, axis=1)\n",
    "    sum_term = tf.multiply(sigma_1_tranformed, max_pool_1)\n",
    "    sum_term_final = tf.expand_dims(sum_term, axis=1)  \n",
    "    \n",
    "    pre_activation_1 = tf.multiply(activation, multipy_pairwise) + sum_term_final\n",
    "    layer_1 = activation_func(pre_activation_1)\n",
    "    return layer_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "#Placeholder for choosing input, epochs, batches, and datasets at runtime\n",
    "learning_rate_class = .01\n",
    "dropout_rate = 0.2\n",
    "set_size = 3\n",
    "    \n",
    "with tf.name_scope('Data_Retrieval'):\n",
    "    #put the data in the graph\n",
    "    batch_size = tf.placeholder_with_default(30, shape=[], name= \"Batch_Size\")\n",
    "    training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "    data_set = tf.placeholder(shape=[None, 3, 10], name=\"All_Data\", dtype=tf.float32)\n",
    "    data_label = tf.placeholder(shape=[None], name=\"Data_Labels\", dtype=tf.int32)  \n",
    "\n",
    "\n",
    "with tf.name_scope(\"BN_Layer_AE_Layers\"):\n",
    "    #Define initalizer and batch normalization layers\n",
    "    bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Set_Analyzer\"):\n",
    "    #the network for generating output of our set\n",
    "    with tf.name_scope('Unique_Identify'):\n",
    "        code_size = 10\n",
    "        n_layer_1 = 100\n",
    "        n_layer_2 = 50\n",
    "        n_layer_3 = 25\n",
    "        n_unq_1 = 15\n",
    "        n_unq_2 = 10\n",
    "        n_unq_final = 2\n",
    "        deep_activation = tf.nn.relu\n",
    "        \n",
    "        batch_item = batch_size\n",
    "        encoding = data_set\n",
    "        \n",
    "        with tf.name_scope('DeepSet_Layer_1'):\n",
    "            #1000\n",
    "            deep_1 = deep_set_layer(encoding, code_size, n_layer_1, set_size=3, layer_name='Deep_One', activation_func=deep_activation)\n",
    "        \n",
    "        with tf.name_scope('DeepSet_Unq_Layer_1'):\n",
    "            #500 Output\n",
    "            deep_unq_1 = deep_set_layer(deep_1, n_layer_1, n_layer_2, set_size=3, layer_name='Deep_Unq_One', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('DeepSet_Unq_Layer_2'):\n",
    "            #250 Output\n",
    "            deep_unq_2 = deep_set_layer(deep_unq_1, n_layer_2, n_layer_3, set_size=3, layer_name='Deep_Unq_Two', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('Final_Unq_Deep_Pool'):\n",
    "            #250 Output\n",
    "            final_unq_deep_layer = tf.reduce_sum(deep_unq_2, 1)\n",
    "            \n",
    "        with tf.name_scope(\"Unq_Hidden_Layer_1\"):\n",
    "            #100 Output\n",
    "            hidden1_unq = tf.layers.dense(final_unq_deep_layer, n_unq_1, name=\"hidden1_unq\", kernel_initializer=he_init)\n",
    "            hidden1_drop_unq = tf.layers.dropout(hidden1_unq, dropout_rate, training=training)\n",
    "            hidden1_cast_unq = tf.cast(hidden1_drop_unq, tf.float32)\n",
    "            bn1_cat_unq = bn_batch_norm_layer(hidden1_cast_unq)\n",
    "            bn1_act_cat_unq = tf.nn.relu(bn1_cat_unq)  \n",
    "            \n",
    "        with tf.name_scope(\"Unq_Hidden_Layer_2\"):\n",
    "            #10 Output\n",
    "            hidden2_unq = tf.layers.dense(bn1_act_cat_unq, n_unq_2, name=\"hidden2_unq\", kernel_initializer=he_init)\n",
    "            hidden2_drop_unq = tf.layers.dropout(hidden2_unq, dropout_rate, training=training)\n",
    "            bn2_cat_unq = bn_batch_norm_layer(hidden2_drop_unq)\n",
    "            bn2_act_cat_unq = tf.nn.relu(bn2_cat_unq)  \n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer_unq\"): \n",
    "            #Get softmax\n",
    "            logits_before_bn_unq = tf.layers.dense(bn2_act_cat_unq, n_unq_final, name=\"outputs_unq\")\n",
    "            logits_unq = bn_batch_norm_layer(logits_before_bn_unq, name=\"logits_unq\")\n",
    "            softmax_unq = tf.nn.softmax(logits_unq, name=\"final_soft_max_unq\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"Unique_loss\"):           \n",
    "            \n",
    "            #Get cross entropy from labels\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=data_label, logits=logits_unq)\n",
    "            loss_unq = tf.reduce_mean(xentropy, name=\"loss_unq\")\n",
    "            loss_summary_unq = tf.summary.scalar('loss_summary_unq', loss_unq)\n",
    "            \n",
    "        with tf.name_scope(\"eval_unq\"):\n",
    "            correct = tf.nn.in_top_k(logits_unq, data_label, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "                \n",
    "        with tf.name_scope(\"unique_train\"):\n",
    "            global_step_unique = tf.Variable(0, trainable=False, name='global_step_unique')\n",
    "            optimizer_unq = tf.train.AdamOptimizer(learning_rate=learning_rate_class)\n",
    "\n",
    "            extra_update_ops_unq = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops_unq):\n",
    "                 training_op_unq = optimizer_unq.minimize(loss_unq, global_step=global_step_unique)\n",
    "\n",
    "init = tf.global_variables_initializer()    \n",
    "saver_total = tf.train.Saver(name=\"Full_Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Graph to log directory\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8226\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the network\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    saver_total.save(sess, set_net_model_best)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/AI/models/set_project_brain_code/logs/set_project_brain_code-run-20190421202911/\n"
     ]
    }
   ],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Train the network to both generate conditioning for the network and also classify the type of set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/set_project_brain_code/model/set_project\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/set_project_brain_code/logs/set_project_brain_code-run-20190421202911/\n",
      "Epoch: 1 Unique Loss: 0.70274377 Accuracy: 0.4\n",
      "Epoch: 2 Unique Loss: 0.69255716 Accuracy: 0.6\n",
      "Epoch: 3 Unique Loss: 0.6789426 Accuracy: 0.6666667\n",
      "Epoch: 4 Unique Loss: 0.6879993 Accuracy: 0.46666667\n",
      "Epoch: 5 Unique Loss: 0.699536 Accuracy: 0.46666667\n",
      "Epoch: 6 Unique Loss: 0.6929773 Accuracy: 0.46666667\n",
      "Epoch: 7 Unique Loss: 0.6922583 Accuracy: 0.53333336\n",
      "Epoch: 8 Unique Loss: 0.6875625 Accuracy: 0.6\n",
      "Epoch: 9 Unique Loss: 0.69512266 Accuracy: 0.53333336\n",
      "Epoch: 10 Unique Loss: 0.69902056 Accuracy: 0.33333334\n",
      "Epoch: 11 Unique Loss: 0.69393593 Accuracy: 0.46666667\n",
      "Epoch: 12 Unique Loss: 0.6920386 Accuracy: 0.6\n",
      "Epoch: 13 Unique Loss: 0.68987674 Accuracy: 0.4\n",
      "Epoch: 14 Unique Loss: 0.69170696 Accuracy: 0.46666667\n",
      "Epoch: 15 Unique Loss: 0.68205184 Accuracy: 0.6\n",
      "Epoch: 16 Unique Loss: 0.6597804 Accuracy: 0.53333336\n",
      "Epoch: 17 Unique Loss: 0.7062863 Accuracy: 0.26666668\n",
      "Epoch: 18 Unique Loss: 0.69019157 Accuracy: 0.53333336\n",
      "Epoch: 19 Unique Loss: 0.651831 Accuracy: 0.73333335\n",
      "Epoch: 20 Unique Loss: 0.64822656 Accuracy: 0.6666667\n",
      "Epoch: 21 Unique Loss: 0.62873024 Accuracy: 0.6\n",
      "Epoch: 22 Unique Loss: 0.7006008 Accuracy: 0.6\n",
      "Epoch: 23 Unique Loss: 0.661504 Accuracy: 0.6\n",
      "Epoch: 24 Unique Loss: 0.6708192 Accuracy: 0.6\n",
      "Epoch: 25 Unique Loss: 0.72791773 Accuracy: 0.46666667\n",
      "Epoch: 26 Unique Loss: 0.6643661 Accuracy: 0.6\n",
      "Epoch: 27 Unique Loss: 0.60428315 Accuracy: 0.8\n",
      "Epoch: 28 Unique Loss: 0.6191377 Accuracy: 0.6666667\n",
      "Epoch: 29 Unique Loss: 0.699062 Accuracy: 0.53333336\n",
      "Epoch: 30 Unique Loss: 0.5818116 Accuracy: 0.6666667\n",
      "Epoch: 31 Unique Loss: 0.71268916 Accuracy: 0.53333336\n",
      "Epoch: 32 Unique Loss: 0.5457113 Accuracy: 0.8\n",
      "Epoch: 33 Unique Loss: 0.44792852 Accuracy: 0.8\n",
      "Epoch: 34 Unique Loss: 0.69700164 Accuracy: 0.6\n",
      "Epoch: 35 Unique Loss: 0.5535449 Accuracy: 0.8\n",
      "Epoch: 36 Unique Loss: 0.510149 Accuracy: 0.8\n",
      "Epoch: 37 Unique Loss: 0.62508875 Accuracy: 0.46666667\n",
      "Epoch: 38 Unique Loss: 0.6325949 Accuracy: 0.6666667\n",
      "Epoch: 39 Unique Loss: 0.46703973 Accuracy: 0.8\n",
      "Epoch: 40 Unique Loss: 0.58181345 Accuracy: 0.6\n",
      "Epoch: 41 Unique Loss: 0.5670817 Accuracy: 0.6666667\n",
      "Epoch: 42 Unique Loss: 0.5870358 Accuracy: 0.6666667\n",
      "Epoch: 43 Unique Loss: 0.6273478 Accuracy: 0.6\n",
      "Epoch: 44 Unique Loss: 0.70175177 Accuracy: 0.6\n",
      "Epoch: 45 Unique Loss: 0.7298799 Accuracy: 0.6\n",
      "Epoch: 46 Unique Loss: 0.56504416 Accuracy: 0.8\n",
      "Epoch: 47 Unique Loss: 0.48034045 Accuracy: 0.6666667\n",
      "Epoch: 48 Unique Loss: 0.4377195 Accuracy: 0.73333335\n",
      "Epoch: 49 Unique Loss: 0.55796707 Accuracy: 0.6666667\n",
      "Epoch: 50 Unique Loss: 0.48171252 Accuracy: 0.8\n",
      "Epoch: 51 Unique Loss: 0.39275151 Accuracy: 0.8666667\n",
      "Epoch: 52 Unique Loss: 0.53121954 Accuracy: 0.8\n",
      "Epoch: 53 Unique Loss: 0.5787008 Accuracy: 0.6666667\n",
      "Epoch: 54 Unique Loss: 0.4379024 Accuracy: 0.8666667\n",
      "Epoch: 55 Unique Loss: 0.6445371 Accuracy: 0.6\n",
      "Epoch: 56 Unique Loss: 0.7021096 Accuracy: 0.53333336\n",
      "Epoch: 57 Unique Loss: 0.606655 Accuracy: 0.6666667\n",
      "Epoch: 58 Unique Loss: 0.67527443 Accuracy: 0.6\n",
      "Epoch: 59 Unique Loss: 0.44917098 Accuracy: 0.8\n",
      "Epoch: 60 Unique Loss: 0.83149934 Accuracy: 0.46666667\n",
      "Epoch: 61 Unique Loss: 0.4260352 Accuracy: 0.8666667\n",
      "Epoch: 62 Unique Loss: 0.5675992 Accuracy: 0.73333335\n",
      "Epoch: 63 Unique Loss: 0.34696025 Accuracy: 0.8\n",
      "Epoch: 64 Unique Loss: 0.48613054 Accuracy: 0.8\n",
      "Epoch: 65 Unique Loss: 0.51482755 Accuracy: 0.73333335\n",
      "Epoch: 66 Unique Loss: 0.41602618 Accuracy: 0.73333335\n",
      "Epoch: 67 Unique Loss: 0.69231075 Accuracy: 0.6\n",
      "Epoch: 68 Unique Loss: 0.5411485 Accuracy: 0.73333335\n",
      "Epoch: 69 Unique Loss: 0.89955944 Accuracy: 0.53333336\n",
      "Epoch: 70 Unique Loss: 0.45420504 Accuracy: 0.73333335\n",
      "Epoch: 71 Unique Loss: 0.4615022 Accuracy: 0.73333335\n",
      "Epoch: 72 Unique Loss: 0.658157 Accuracy: 0.6666667\n",
      "Epoch: 73 Unique Loss: 0.5620627 Accuracy: 0.6666667\n",
      "Epoch: 74 Unique Loss: 0.5659486 Accuracy: 0.6666667\n",
      "Epoch: 75 Unique Loss: 0.48109257 Accuracy: 0.8\n",
      "Epoch: 76 Unique Loss: 0.61961883 Accuracy: 0.8\n",
      "Epoch: 77 Unique Loss: 0.36738363 Accuracy: 0.8666667\n",
      "Epoch: 78 Unique Loss: 0.2782434 Accuracy: 0.8666667\n",
      "Epoch: 79 Unique Loss: 0.28469855 Accuracy: 0.8666667\n",
      "Epoch: 80 Unique Loss: 0.462453 Accuracy: 0.6666667\n",
      "Epoch: 81 Unique Loss: 0.6926518 Accuracy: 0.73333335\n",
      "Epoch: 82 Unique Loss: 0.5002866 Accuracy: 0.73333335\n",
      "Epoch: 83 Unique Loss: 0.52647763 Accuracy: 0.73333335\n",
      "Epoch: 84 Unique Loss: 0.5632612 Accuracy: 0.73333335\n",
      "Epoch: 85 Unique Loss: 0.7164192 Accuracy: 0.6666667\n",
      "Epoch: 86 Unique Loss: 0.37982666 Accuracy: 0.8666667\n",
      "Epoch: 87 Unique Loss: 0.5605007 Accuracy: 0.73333335\n",
      "Epoch: 88 Unique Loss: 0.37743944 Accuracy: 0.8666667\n",
      "Epoch: 89 Unique Loss: 0.49886653 Accuracy: 0.8\n",
      "Epoch: 90 Unique Loss: 0.95710415 Accuracy: 0.6\n",
      "Epoch: 91 Unique Loss: 0.93310636 Accuracy: 0.46666667\n",
      "Epoch: 92 Unique Loss: 0.4377489 Accuracy: 0.8\n",
      "Epoch: 93 Unique Loss: 0.2872439 Accuracy: 0.8666667\n",
      "Epoch: 94 Unique Loss: 0.5077097 Accuracy: 0.8\n",
      "Epoch: 95 Unique Loss: 0.3463914 Accuracy: 0.8666667\n",
      "Epoch: 96 Unique Loss: 0.4069579 Accuracy: 0.73333335\n",
      "Epoch: 97 Unique Loss: 0.8358935 Accuracy: 0.73333335\n",
      "Epoch: 98 Unique Loss: 0.39292654 Accuracy: 0.8666667\n",
      "Epoch: 99 Unique Loss: 0.6907406 Accuracy: 0.73333335\n",
      "Epoch: 100 Unique Loss: 0.33312216 Accuracy: 0.8666667\n",
      "Epoch: 101 Unique Loss: 0.6527233 Accuracy: 0.73333335\n",
      "Epoch: 102 Unique Loss: 0.98331666 Accuracy: 0.6666667\n",
      "Epoch: 103 Unique Loss: 0.45968825 Accuracy: 0.73333335\n",
      "Epoch: 104 Unique Loss: 0.40283287 Accuracy: 0.8\n",
      "Epoch: 105 Unique Loss: 0.72547597 Accuracy: 0.6\n",
      "Epoch: 106 Unique Loss: 0.5557114 Accuracy: 0.8666667\n",
      "Epoch: 107 Unique Loss: 0.47605312 Accuracy: 0.8\n",
      "Epoch: 108 Unique Loss: 0.2694254 Accuracy: 0.8666667\n",
      "Epoch: 109 Unique Loss: 0.58931935 Accuracy: 0.8\n",
      "Epoch: 110 Unique Loss: 0.2867614 Accuracy: 0.8666667\n",
      "Epoch: 111 Unique Loss: 0.16280803 Accuracy: 0.93333334\n",
      "Epoch: 112 Unique Loss: 0.28246152 Accuracy: 0.8666667\n",
      "Epoch: 113 Unique Loss: 0.26203915 Accuracy: 0.93333334\n",
      "Epoch: 114 Unique Loss: 0.49483314 Accuracy: 0.8\n",
      "Epoch: 115 Unique Loss: 0.8491646 Accuracy: 0.73333335\n",
      "Epoch: 116 Unique Loss: 0.96109635 Accuracy: 0.6\n",
      "Epoch: 117 Unique Loss: 0.3996276 Accuracy: 0.8666667\n",
      "Epoch: 118 Unique Loss: 0.42061546 Accuracy: 0.8\n",
      "Epoch: 119 Unique Loss: 0.1729858 Accuracy: 0.93333334\n",
      "Epoch: 120 Unique Loss: 0.33245295 Accuracy: 0.8666667\n",
      "Epoch: 121 Unique Loss: 0.3289112 Accuracy: 0.8666667\n",
      "Epoch: 122 Unique Loss: 0.31446362 Accuracy: 0.8666667\n",
      "Epoch: 123 Unique Loss: 0.40930942 Accuracy: 0.8\n",
      "Epoch: 124 Unique Loss: 0.88553685 Accuracy: 0.6\n",
      "Epoch: 125 Unique Loss: 0.18729964 Accuracy: 0.8666667\n",
      "Epoch: 126 Unique Loss: 1.0072796 Accuracy: 0.6666667\n",
      "Epoch: 127 Unique Loss: 0.71327525 Accuracy: 0.6666667\n",
      "Epoch: 128 Unique Loss: 0.77618927 Accuracy: 0.8\n",
      "Epoch: 129 Unique Loss: 0.84353447 Accuracy: 0.8\n",
      "Epoch: 130 Unique Loss: 0.8685194 Accuracy: 0.6666667\n",
      "Epoch: 131 Unique Loss: 0.06873102 Accuracy: 1.0\n",
      "Epoch: 132 Unique Loss: 0.61125857 Accuracy: 0.8\n",
      "Epoch: 133 Unique Loss: 0.42553985 Accuracy: 0.8\n",
      "Epoch: 134 Unique Loss: 0.13268499 Accuracy: 0.93333334\n",
      "Epoch: 135 Unique Loss: 1.0008514 Accuracy: 0.8\n",
      "Epoch: 136 Unique Loss: 0.354116 Accuracy: 0.8\n",
      "Epoch: 137 Unique Loss: 0.25810257 Accuracy: 0.8666667\n",
      "Epoch: 138 Unique Loss: 0.28800198 Accuracy: 0.8666667\n",
      "Epoch: 139 Unique Loss: 0.87263507 Accuracy: 0.6\n",
      "Epoch: 140 Unique Loss: 0.595151 Accuracy: 0.8\n",
      "Epoch: 141 Unique Loss: 0.70776784 Accuracy: 0.6666667\n",
      "Epoch: 142 Unique Loss: 0.5541954 Accuracy: 0.8\n",
      "Epoch: 143 Unique Loss: 0.48705718 Accuracy: 0.8\n",
      "Epoch: 144 Unique Loss: 0.16644941 Accuracy: 0.93333334\n",
      "Epoch: 145 Unique Loss: 0.24806568 Accuracy: 0.8666667\n",
      "Epoch: 146 Unique Loss: 0.35144112 Accuracy: 0.8\n",
      "Epoch: 147 Unique Loss: 0.39895126 Accuracy: 0.73333335\n",
      "Epoch: 148 Unique Loss: 0.31980103 Accuracy: 0.8666667\n",
      "Epoch: 149 Unique Loss: 0.3007053 Accuracy: 0.8666667\n",
      "Epoch: 150 Unique Loss: 0.45975378 Accuracy: 0.8\n",
      "Epoch: 151 Unique Loss: 0.22043505 Accuracy: 0.8666667\n",
      "Epoch: 152 Unique Loss: 0.6532687 Accuracy: 0.8\n",
      "Epoch: 153 Unique Loss: 0.17479141 Accuracy: 0.93333334\n",
      "Epoch: 154 Unique Loss: 0.4915654 Accuracy: 0.8\n",
      "Epoch: 155 Unique Loss: 0.63954055 Accuracy: 0.6666667\n",
      "Epoch: 156 Unique Loss: 0.44117805 Accuracy: 0.73333335\n",
      "Epoch: 157 Unique Loss: 0.46759644 Accuracy: 0.8\n",
      "Epoch: 158 Unique Loss: 0.77531326 Accuracy: 0.6666667\n",
      "Epoch: 159 Unique Loss: 0.7357958 Accuracy: 0.8\n",
      "Epoch: 160 Unique Loss: 0.021341976 Accuracy: 1.0\n",
      "Epoch: 161 Unique Loss: 0.77909833 Accuracy: 0.8\n",
      "Epoch: 162 Unique Loss: 0.33700177 Accuracy: 0.8\n",
      "Epoch: 163 Unique Loss: 0.38305986 Accuracy: 0.8\n",
      "Epoch: 164 Unique Loss: 1.1311295 Accuracy: 0.6666667\n",
      "Epoch: 165 Unique Loss: 0.7253508 Accuracy: 0.73333335\n",
      "Epoch: 166 Unique Loss: 0.9555883 Accuracy: 0.6666667\n",
      "Epoch: 167 Unique Loss: 0.46044782 Accuracy: 0.73333335\n",
      "Epoch: 168 Unique Loss: 0.42695644 Accuracy: 0.8\n",
      "Epoch: 169 Unique Loss: 0.47764003 Accuracy: 0.8666667\n",
      "Epoch: 170 Unique Loss: 0.34272337 Accuracy: 0.8\n",
      "Epoch: 171 Unique Loss: 0.42252624 Accuracy: 0.8666667\n",
      "Epoch: 172 Unique Loss: 0.62967706 Accuracy: 0.8\n",
      "Epoch: 173 Unique Loss: 0.31491068 Accuracy: 0.8666667\n",
      "Epoch: 174 Unique Loss: 0.36172572 Accuracy: 0.8\n",
      "Epoch: 175 Unique Loss: 0.14406163 Accuracy: 0.93333334\n",
      "Epoch: 176 Unique Loss: 0.36071217 Accuracy: 0.8666667\n",
      "Epoch: 177 Unique Loss: 0.53068775 Accuracy: 0.8666667\n",
      "Epoch: 178 Unique Loss: 0.71550786 Accuracy: 0.6666667\n",
      "Epoch: 179 Unique Loss: 0.46776313 Accuracy: 0.8\n",
      "Epoch: 180 Unique Loss: 0.8935891 Accuracy: 0.8\n",
      "Epoch: 181 Unique Loss: 0.2973013 Accuracy: 0.8666667\n",
      "Epoch: 182 Unique Loss: 0.5029539 Accuracy: 0.8666667\n",
      "Epoch: 183 Unique Loss: 0.34279796 Accuracy: 0.93333334\n",
      "Epoch: 184 Unique Loss: 0.68262464 Accuracy: 0.73333335\n",
      "Epoch: 185 Unique Loss: 0.17658094 Accuracy: 0.93333334\n",
      "Epoch: 186 Unique Loss: 0.52818334 Accuracy: 0.6\n",
      "Epoch: 187 Unique Loss: 0.56086314 Accuracy: 0.8\n",
      "Epoch: 188 Unique Loss: 0.42899504 Accuracy: 0.73333335\n",
      "Epoch: 189 Unique Loss: 0.536204 Accuracy: 0.93333334\n",
      "Epoch: 190 Unique Loss: 1.0917526 Accuracy: 0.6\n",
      "Epoch: 191 Unique Loss: 0.2863551 Accuracy: 0.8\n",
      "Epoch: 192 Unique Loss: 0.17965569 Accuracy: 0.93333334\n",
      "Epoch: 193 Unique Loss: 0.80567497 Accuracy: 0.8\n",
      "Epoch: 194 Unique Loss: 0.81237704 Accuracy: 0.6666667\n",
      "Epoch: 195 Unique Loss: 0.6381104 Accuracy: 0.8\n",
      "Epoch: 196 Unique Loss: 0.44135568 Accuracy: 0.8\n",
      "Epoch: 197 Unique Loss: 0.4551832 Accuracy: 0.8666667\n",
      "Epoch: 198 Unique Loss: 0.3287485 Accuracy: 0.93333334\n",
      "Epoch: 199 Unique Loss: 0.58282465 Accuracy: 0.73333335\n",
      "Epoch: 200 Unique Loss: 1.0675995 Accuracy: 0.73333335\n",
      "Epoch: 201 Unique Loss: 0.42371687 Accuracy: 0.8\n",
      "Epoch: 202 Unique Loss: 0.88645464 Accuracy: 0.73333335\n",
      "Epoch: 203 Unique Loss: 0.7740537 Accuracy: 0.8\n",
      "Epoch: 204 Unique Loss: 0.3608888 Accuracy: 0.8\n",
      "Epoch: 205 Unique Loss: 0.34320685 Accuracy: 0.8666667\n",
      "Epoch: 206 Unique Loss: 0.43852058 Accuracy: 0.8\n",
      "Epoch: 207 Unique Loss: 0.3777405 Accuracy: 0.8\n",
      "Epoch: 208 Unique Loss: 0.3336339 Accuracy: 0.8666667\n",
      "Epoch: 209 Unique Loss: 0.3354868 Accuracy: 0.8\n",
      "Epoch: 210 Unique Loss: 0.162561 Accuracy: 0.93333334\n",
      "Epoch: 211 Unique Loss: 0.29471332 Accuracy: 0.8666667\n",
      "Epoch: 212 Unique Loss: 0.09070361 Accuracy: 1.0\n",
      "Epoch: 213 Unique Loss: 0.37933126 Accuracy: 0.93333334\n",
      "Epoch: 214 Unique Loss: 0.37387177 Accuracy: 0.73333335\n",
      "Epoch: 215 Unique Loss: 0.4519625 Accuracy: 0.8\n",
      "Epoch: 216 Unique Loss: 0.6828371 Accuracy: 0.8\n",
      "Epoch: 217 Unique Loss: 0.7762147 Accuracy: 0.6666667\n",
      "Epoch: 218 Unique Loss: 0.6378222 Accuracy: 0.8666667\n",
      "Epoch: 219 Unique Loss: 0.44824 Accuracy: 0.73333335\n",
      "Epoch: 220 Unique Loss: 0.511283 Accuracy: 0.8\n",
      "Epoch: 221 Unique Loss: 0.13438521 Accuracy: 0.93333334\n",
      "Epoch: 222 Unique Loss: 0.24317005 Accuracy: 0.8666667\n",
      "Epoch: 223 Unique Loss: 0.11477087 Accuracy: 0.93333334\n",
      "Epoch: 224 Unique Loss: 0.51457554 Accuracy: 0.73333335\n",
      "Epoch: 225 Unique Loss: 1.3056539 Accuracy: 0.6666667\n",
      "Epoch: 226 Unique Loss: 0.045678124 Accuracy: 1.0\n",
      "Epoch: 227 Unique Loss: 0.37774017 Accuracy: 0.8666667\n",
      "Epoch: 228 Unique Loss: 0.19018038 Accuracy: 0.93333334\n",
      "Epoch: 229 Unique Loss: 0.10037234 Accuracy: 1.0\n",
      "Epoch: 230 Unique Loss: 0.19254705 Accuracy: 0.93333334\n",
      "Epoch: 231 Unique Loss: 1.0187049 Accuracy: 0.73333335\n",
      "Epoch: 232 Unique Loss: 1.2735447 Accuracy: 0.6\n",
      "Epoch: 233 Unique Loss: 1.1178988 Accuracy: 0.6\n",
      "Epoch: 234 Unique Loss: 0.17078905 Accuracy: 0.8666667\n",
      "Epoch: 235 Unique Loss: 0.1533509 Accuracy: 0.93333334\n",
      "Epoch: 236 Unique Loss: 0.25017396 Accuracy: 0.93333334\n",
      "Epoch: 237 Unique Loss: 0.89062667 Accuracy: 0.73333335\n",
      "Epoch: 238 Unique Loss: 0.42871946 Accuracy: 0.8\n",
      "Epoch: 239 Unique Loss: 0.5099415 Accuracy: 0.73333335\n",
      "Epoch: 240 Unique Loss: 0.2938059 Accuracy: 0.8666667\n",
      "Epoch: 241 Unique Loss: 0.15853722 Accuracy: 0.93333334\n",
      "Epoch: 242 Unique Loss: 0.5512118 Accuracy: 0.8\n",
      "Epoch: 243 Unique Loss: 0.34005818 Accuracy: 0.8666667\n",
      "Epoch: 244 Unique Loss: 0.2866569 Accuracy: 0.8666667\n",
      "Epoch: 245 Unique Loss: 0.25258082 Accuracy: 0.8666667\n",
      "Epoch: 246 Unique Loss: 0.5310312 Accuracy: 0.8\n",
      "Epoch: 247 Unique Loss: 0.62862474 Accuracy: 0.8\n",
      "Epoch: 248 Unique Loss: 0.71023303 Accuracy: 0.8666667\n",
      "Epoch: 249 Unique Loss: 0.24575129 Accuracy: 0.93333334\n",
      "Did 9960 of loss minimized training in 65.00999975204468 seconds.\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "steps_between_test_save = 1\n",
    "batch = 15\n",
    "train_size = 600\n",
    "#all_data_steps = np.int(np.floor(train_size/batch))\n",
    "all_data_steps = 40\n",
    "worst_acc = 0\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_total.restore(sess, set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 1\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "            idx = np.random.randint(full_train.shape[0], size=batch)\n",
    "            batch_train = full_train[idx,:]\n",
    "            batch_train_label = full_train_labels[idx]\n",
    "            \n",
    "            #run Training Op\n",
    "            sess.run([training_op_unq], feed_dict={data_set: batch_train, data_label: batch_train_label})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0) :\n",
    "            idx = np.random.randint(full_test.shape[0], size=batch)\n",
    "            batch_test = full_test[idx,:]\n",
    "            batch_test_label = full_test_labels[idx]\n",
    "            \n",
    "            loss_un, loss_un_val, acc, acc_sum = sess.run([loss_summary_unq, loss_unq, accuracy, accuracy_summary], \n",
    "                                                   feed_dict = {data_set: batch_test , data_label: batch_test_label, training: False, batch_size: batch})\n",
    "\n",
    "            filewriter.add_summary(loss_un, step)\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Unique Loss: \" + str(loss_un_val) + \" Accuracy: \" + str(acc))\n",
    "            if acc > worst_acc:\n",
    "                saver_total.save(sess, set_net_model_best)\n",
    "                worst_acc = acc\n",
    "            saver_total.save(sess, set_net_model)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step_unique)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Training Metrics\n",
    "\n",
    "Here we will get final accuracies for both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/set_project_brain_code/model/set_project\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/set_project_brain_code/logs/set_project_brain_code-run-20190421202911/\n",
      "Final Accuracy: 0.8343749903142452\n"
     ]
    }
   ],
   "source": [
    "batch = 20\n",
    "test_size = full_test.shape[0]\n",
    "all_data_steps = np.int(np.floor(test_size/batch))\n",
    "unique_accuracy = 0\n",
    "start_index = 0\n",
    "end_index = batch\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_total.restore(sess, set_net_model)\n",
    "    \n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    for i in range(all_data_steps):\n",
    "        idx = np.arange(start_index, end_index)\n",
    "        batch_test = full_test[idx,:]\n",
    "        batch_test_label = full_test_labels[idx]\n",
    "        unq_acc = sess.run(accuracy, feed_dict = {data_set: batch_test , data_label: batch_test_label, training: False, batch_size: batch})\n",
    "        unique_accuracy = ((i  * unique_accuracy) + unq_acc)/(i + 1)\n",
    "        start_index = start_index + batch\n",
    "        end_index = end_index + batch\n",
    "end_time = time.time()\n",
    "print(\"Final Accuracy: \" + str(unique_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8343749903142452"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
