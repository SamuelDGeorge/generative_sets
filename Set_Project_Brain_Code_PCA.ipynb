{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import reset_graph\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tan.tan_util import get_tan_nll as tan\n",
    "from tan.tan_util import get_tan_nll_cond as tan_cond\n",
    "\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import imagenet_helper_files.vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "#For parsing records once written\n",
    "from Utilities.set_record_parser import build_set_dataset\n",
    "from Utilities.set_record_parser import get_file_lists\n",
    "from Utilities.models import log_dir_build\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Directories \n",
    "\n",
    "Here we are going to get the files needed to do the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate Neccesary Files\n",
    "homogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Original_Images_Train.pkl\", \"rb\")\n",
    "nonhomogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Inhomogeneous_Images_Train.pkl\", \"rb\")\n",
    "homogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Original_Images_Test.pkl\", \"rb\")\n",
    "nonhomogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/L/Inhomogeneous_Images_Test.pkl\", \"rb\")\n",
    "train_homog_l = pickle.load(homogenous_train)\n",
    "train_homog_labels_l = np.zeros(train_homog_l.shape[0])\n",
    "train_nonhomog_l = pickle.load(nonhomogenous_train)\n",
    "train_nonhomog_labels_l = np.ones(train_nonhomog_l.shape[0])\n",
    "test_homog_l = pickle.load(homogenous_test)\n",
    "test_homog_labels_l = np.zeros(test_homog_l.shape[0])\n",
    "test_nonhomog_l = pickle.load(nonhomogenous_test)\n",
    "test_nonhomog_labels_l = np.ones(test_nonhomog_l.shape[0])\n",
    "homogenous_train.close()\n",
    "nonhomogenous_train.close()\n",
    "homogenous_test.close()\n",
    "nonhomogenous_test.close()\n",
    "#Locate Neccesary Files\n",
    "homogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Original_Images_Train.pkl\", \"rb\")\n",
    "nonhomogenous_train = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Inhomogeneous_Images_Train.pkl\", \"rb\")\n",
    "homogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Original_Images_Test.pkl\", \"rb\")\n",
    "nonhomogenous_test = open(\"D:/Machine_Learning/Datasets/cortical_images/R/Inhomogeneous_Images_Test.pkl\", \"rb\")\n",
    "train_homog_r = pickle.load(homogenous_train)\n",
    "train_homog_labels_r = np.zeros(train_homog_r.shape[0])\n",
    "train_nonhomog_r = pickle.load(nonhomogenous_train)\n",
    "train_nonhomog_labels_r = np.ones(train_nonhomog_r.shape[0])\n",
    "test_homog_r = pickle.load(homogenous_test)\n",
    "test_homog_labels_r = np.zeros(test_homog_r.shape[0])\n",
    "test_nonhomog_r = pickle.load(nonhomogenous_test)\n",
    "test_nonhomog_labels_r = np.ones(test_nonhomog_r.shape[0])\n",
    "homogenous_train.close()\n",
    "nonhomogenous_train.close()\n",
    "homogenous_test.close()\n",
    "nonhomogenous_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine into full train and test set as well as ground truth\n",
    "full_train = np.concatenate((train_homog_l, train_nonhomog_l, train_homog_r, train_nonhomog_r), axis = 0)\n",
    "full_train_labels = np.concatenate((train_homog_labels_l, train_nonhomog_labels_l, train_homog_labels_r, train_nonhomog_labels_r), axis=0)\n",
    "\n",
    "full_test = np.concatenate((test_homog_l, test_nonhomog_l, test_homog_r, test_nonhomog_r), axis = 0)\n",
    "full_test_labels = np.concatenate((test_homog_labels_l, test_nonhomog_labels_l, test_homog_labels_r, test_nonhomog_labels_r), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/sdgeo/Dropbox/Own/Graduate-School/Second_Semester/Comp_790/generative_sets\n"
     ]
    }
   ],
   "source": [
    "image_1 = full_test[0]\n",
    "full_test.shape\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show and Image from set\n",
    "x_val = full_test[0][2]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.savefig('Brain_3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Neural Network\n",
    "\n",
    "The following provides the code to import and use the TF_Records for the set project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for logs in training\n",
    "set_net_logs = 'D:/AI/models/set_project_brain_code_pca/logs'\n",
    "model_path = log_dir_build(set_net_logs, \"set_project_brain_code_pca\")\n",
    "#model_path = 'D:/AI/models/set_project_brain/logs/set_project-run-20190306174848/'\n",
    "\n",
    "#directory for all the models saved during training\n",
    "pca_model = 'D:/AI/models/set_project_brain_code_pca/model/' + 'set_pca'\n",
    "set_net_model = 'D:/AI/models/set_project_brain_code_pca/model/' + 'set_project'\n",
    "set_net_model_best = 'D:/AI/models/set_project_brain_code_pca/model/' + 'set_project_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_set_layer(input_code, input_size, condense_size, set_size=3, layer_name='default', activation_func=tf.nn.sigmoid):\n",
    "    \n",
    "    learned_transform = tf.get_variable(layer_name + '_transform', shape=[input_size,condense_size], \n",
    "                                        trainable=True, initializer=tf.contrib.layers.variance_scaling_initializer()) \n",
    "    batched_transform = tf.broadcast_to(learned_transform, [tf.shape(input_code)[0], input_size, condense_size])\n",
    "    transform_layer = tf.matmul(input_code, batched_transform)\n",
    "    activation = activation_func(transform_layer)\n",
    "    \n",
    "    \n",
    "    lambda_1 = tf.get_variable(layer_name + \"_lambda\", [condense_size], trainable=True, dtype=tf.float32, initializer=tf.initializers.random_normal(mean=1))\n",
    "    lambda_1_transformed = tf.broadcast_to(lambda_1, [set_size, condense_size])\n",
    "    multipy_pairwise = tf.broadcast_to(lambda_1_transformed, [tf.shape(input_code)[0], set_size, condense_size])\n",
    "    \n",
    "    sigma_1 = tf.abs(tf.get_variable(layer_name + \"_sigma\", [condense_size], trainable=True, dtype=tf.float32, initializer=tf.initializers.random_normal(mean=0)))\n",
    "    sigma_1_tranformed = tf.broadcast_to(sigma_1, [tf.shape(input_code)[0], condense_size])\n",
    "    \n",
    "\n",
    "    # + sigma * mean(Data)\n",
    "    max_pool_1 = tf.reduce_mean(activation, axis=1)\n",
    "    sum_term = tf.multiply(sigma_1_tranformed, max_pool_1)\n",
    "    sum_term_final = tf.expand_dims(sum_term, axis=1)  \n",
    "    \n",
    "    pre_activation_1 = tf.multiply(activation, multipy_pairwise) + sum_term_final\n",
    "    layer_1 = activation_func(pre_activation_1)\n",
    "    return layer_1\n",
    "\n",
    "def PCA_Network(Input_Data, reuse = None):\n",
    "    with tf.variable_scope(\"PCA\", reuse=reuse):\n",
    "        code_size = 10\n",
    "        pca_learning_rate = 1\n",
    "        code_layer_size = 16384\n",
    "        with tf.name_scope(\"Original_Coding\"):\n",
    "            pca_code_layer = tf.layers.flatten(Input_Data)\n",
    "            batch_mean = tf.expand_dims(tf.reduce_mean(pca_code_layer,0),0)\n",
    "            pca_code_normalized = tf.subtract(pca_code_layer, batch_mean)\n",
    "\n",
    "        with tf.name_scope(\"PCA_Layer\"):\n",
    "            reduction_matrix = tf.get_variable(\"weights\", shape = [code_layer_size, code_size], initializer = tf.random_normal_initializer())\n",
    "            Code = tf.matmul(pca_code_normalized,reduction_matrix, name=\"multipy\")\n",
    "\n",
    "        with tf.name_scope(\"Reconstruction_Layer_Final\"):\n",
    "            final_reconstruction_layer_bn = tf.matmul(Code, tf.transpose(reduction_matrix))\n",
    "            final_reconstruction_layer = tf.add(final_reconstruction_layer_bn,batch_mean)\n",
    "\n",
    "        with tf.name_scope(\"PCA_Loss\"):\n",
    "            pca_loss = tf.losses.mean_squared_error(final_reconstruction_layer,pca_code_layer)\n",
    "            pca_loss_summary = tf.summary.scalar('pca_loss_summary', pca_loss)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"PCA_Train\"):\n",
    "            pca_optimizer = tf.train.AdamOptimizer(learning_rate=pca_learning_rate, name='pca_optomizer')\n",
    "            pca_global_step = tf.Variable(0, trainable=False, name='pca_global_step')\n",
    "            training_op_pca = pca_optimizer.minimize(pca_loss, global_step=pca_global_step, name=\"training_op\")\n",
    "\n",
    "        saver_pca = tf.train.Saver(name=\"PCA_Saver\")\n",
    "        return Code, pca_loss, pca_loss_summary, training_op_pca, saver_pca\n",
    "\n",
    "def PCA_Trainer(Input_Data):\n",
    "    Code, pca_loss, pca_loss_summary, training_op_pca, saver_pca = PCA_Network(Input_Data, reuse = tf.AUTO_REUSE)\n",
    "    return Code, pca_loss, pca_loss_summary, training_op_pca, saver_pca\n",
    "\n",
    "def PCA_encode(Input_Data):\n",
    "    Code, pca_loss, pca_loss_summary, training_op_pca, saver_pca = PCA_Network(Input_Data, reuse = tf.AUTO_REUSE)\n",
    "    return Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "#Placeholder for choosing input, epochs, batches, and datasets at runtime\n",
    "learning_rate_class = .01\n",
    "dropout_rate = 0.2\n",
    "set_size = 3\n",
    "    \n",
    "with tf.name_scope('Data_Retrieval'):\n",
    "    #put the data in the graph\n",
    "    batch_size = tf.placeholder_with_default(30, shape=[], name= \"Batch_Size\")\n",
    "    training = tf.placeholder_with_default(True, shape=(), name = 'training')\n",
    "    \n",
    "    data_set = tf.placeholder(shape=[None, 3, 128, 128], name=\"All_Data\", dtype=tf.float32)\n",
    "    single_data = tf.placeholder(shape=[None, 128, 128], name=\"Single_Data\", dtype=tf.float32)\n",
    "    data_label = tf.placeholder(shape=[None], name=\"Data_Labels\", dtype=tf.int32)  \n",
    "\n",
    "    \n",
    "with tf.variable_scope(\"PCA_Code_Maker\"):\n",
    "    Code, pca_loss, pca_loss_summary, training_op_pca, saver_pca = PCA_Trainer(single_data)\n",
    "    pca_code = tf.map_fn(lambda x: PCA_encode(x), data_set)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"BN_Layer_AE_Layers\"):\n",
    "    #Define initalizer and batch normalization layers\n",
    "    bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()    \n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Set_Analyzer\"):\n",
    "    #the network for generating output of our set\n",
    "    with tf.name_scope('Unique_Identify'):\n",
    "        code_size = 10\n",
    "        n_layer_1 = 100\n",
    "        n_layer_2 = 50\n",
    "        n_layer_3 = 25\n",
    "        n_unq_1 = 15\n",
    "        n_unq_2 = 10\n",
    "        n_unq_final = 2\n",
    "        deep_activation = tf.nn.relu\n",
    "        \n",
    "        batch_item = batch_size\n",
    "        encoding = tf.stop_gradient(pca_code)\n",
    "        \n",
    "        with tf.name_scope('DeepSet_Layer_1'):\n",
    "            #1000\n",
    "            deep_1 = deep_set_layer(encoding, code_size, n_layer_1, set_size=3, layer_name='Deep_One', activation_func=deep_activation)\n",
    "        \n",
    "        with tf.name_scope('DeepSet_Unq_Layer_1'):\n",
    "            #500 Output\n",
    "            deep_unq_1 = deep_set_layer(deep_1, n_layer_1, n_layer_2, set_size=3, layer_name='Deep_Unq_One', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('DeepSet_Unq_Layer_2'):\n",
    "            #250 Output\n",
    "            deep_unq_2 = deep_set_layer(deep_unq_1, n_layer_2, n_layer_3, set_size=3, layer_name='Deep_Unq_Two', activation_func=deep_activation)\n",
    "            \n",
    "        with tf.name_scope('Final_Unq_Deep_Pool'):\n",
    "            #250 Output\n",
    "            final_unq_deep_layer = tf.reduce_sum(deep_unq_2, 1)\n",
    "            \n",
    "        with tf.name_scope(\"Unq_Hidden_Layer_1\"):\n",
    "            #100 Output\n",
    "            hidden1_unq = tf.layers.dense(final_unq_deep_layer, n_unq_1, name=\"hidden1_unq\", kernel_initializer=he_init)\n",
    "            hidden1_drop_unq = tf.layers.dropout(hidden1_unq, dropout_rate, training=training)\n",
    "            hidden1_cast_unq = tf.cast(hidden1_drop_unq, tf.float32)\n",
    "            bn1_cat_unq = bn_batch_norm_layer(hidden1_cast_unq)\n",
    "            bn1_act_cat_unq = tf.nn.relu(bn1_cat_unq)  \n",
    "            \n",
    "        with tf.name_scope(\"Unq_Hidden_Layer_2\"):\n",
    "            #10 Output\n",
    "            hidden2_unq = tf.layers.dense(bn1_act_cat_unq, n_unq_2, name=\"hidden2_unq\", kernel_initializer=he_init)\n",
    "            hidden2_drop_unq = tf.layers.dropout(hidden2_unq, dropout_rate, training=training)\n",
    "            bn2_cat_unq = bn_batch_norm_layer(hidden2_drop_unq)\n",
    "            bn2_act_cat_unq = tf.nn.relu(bn2_cat_unq)  \n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Final_Layer_unq\"): \n",
    "            #Get softmax\n",
    "            logits_before_bn_unq = tf.layers.dense(bn2_act_cat_unq, n_unq_final, name=\"outputs_unq\")\n",
    "            logits_unq = bn_batch_norm_layer(logits_before_bn_unq, name=\"logits_unq\")\n",
    "            softmax_unq = tf.nn.softmax(logits_unq, name=\"final_soft_max_unq\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"Unique_loss\"):           \n",
    "            \n",
    "            #Get cross entropy from labels\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=data_label, logits=logits_unq)\n",
    "            loss_unq = tf.reduce_mean(xentropy, name=\"loss_unq\")\n",
    "            loss_summary_unq = tf.summary.scalar('loss_summary_unq', loss_unq)\n",
    "            \n",
    "        with tf.name_scope(\"eval_unq\"):\n",
    "            correct = tf.nn.in_top_k(logits_unq, data_label, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy_summary', accuracy)\n",
    "                \n",
    "        with tf.name_scope(\"unique_train\"):\n",
    "            global_step_unique = tf.Variable(0, trainable=False, name='global_step_unique')\n",
    "            optimizer_unq = tf.train.AdamOptimizer(learning_rate=learning_rate_class)\n",
    "\n",
    "            extra_update_ops_unq = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "            with tf.control_dependencies(extra_update_ops_unq):\n",
    "                 training_op_unq = optimizer_unq.minimize(loss_unq, global_step=global_step_unique)\n",
    "\n",
    "init = tf.global_variables_initializer()    \n",
    "saver_total = tf.train.Saver(name=\"Full_Graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Graph to log directory\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172066\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the network\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    saver_total.save(sess, set_net_model_best)\n",
    "    saver_pca.save(sess, pca_model)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/AI/models/set_project_brain_code_pca/logs/set_project_brain_code_pca-run-20190422003209/\n"
     ]
    }
   ],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the PCA\n",
    "\n",
    "Here we are going to train the network to do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/set_project_brain_code_pca/model/set_project\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/set_project_brain_code_pca/logs/set_project_brain_code_pca-run-20190422003209/\n",
      "Epoch: 5 PCA Loss: 4999.9756\n",
      "Epoch: 10 PCA Loss: 1494.2421\n",
      "Epoch: 15 PCA Loss: 1064.8932\n",
      "Epoch: 20 PCA Loss: 515.82556\n",
      "Epoch: 25 PCA Loss: 581.9608\n",
      "Epoch: 30 PCA Loss: 435.66928\n",
      "Epoch: 35 PCA Loss: 303.86703\n",
      "Epoch: 40 PCA Loss: 354.54156\n",
      "Epoch: 45 PCA Loss: 357.5078\n",
      "Epoch: 50 PCA Loss: 294.1342\n",
      "Epoch: 55 PCA Loss: 3157.3489\n",
      "Epoch: 60 PCA Loss: 180.91803\n",
      "Epoch: 65 PCA Loss: 166.3665\n",
      "Epoch: 70 PCA Loss: 1223.5214\n",
      "Epoch: 75 PCA Loss: 98.55482\n",
      "Epoch: 80 PCA Loss: 97.356415\n",
      "Epoch: 85 PCA Loss: 56.87899\n",
      "Epoch: 90 PCA Loss: 50.761623\n",
      "Epoch: 95 PCA Loss: 63.909214\n",
      "Epoch: 100 PCA Loss: 48.91504\n",
      "Epoch: 105 PCA Loss: 39.03207\n",
      "Epoch: 110 PCA Loss: 33.43456\n",
      "Epoch: 115 PCA Loss: 47.927692\n",
      "Epoch: 120 PCA Loss: 34.47819\n",
      "Epoch: 125 PCA Loss: 121.79633\n",
      "Epoch: 130 PCA Loss: 33.10301\n",
      "Epoch: 135 PCA Loss: 30.266838\n",
      "Epoch: 140 PCA Loss: 32.111404\n",
      "Epoch: 145 PCA Loss: 33.623207\n",
      "Epoch: 150 PCA Loss: 16.740389\n",
      "Epoch: 155 PCA Loss: 27.751526\n",
      "Epoch: 160 PCA Loss: 51.424145\n",
      "Epoch: 165 PCA Loss: 14.195705\n",
      "Epoch: 170 PCA Loss: 12.843158\n",
      "Epoch: 175 PCA Loss: 12.497569\n",
      "Epoch: 180 PCA Loss: 107.60744\n",
      "Epoch: 185 PCA Loss: 4.233908\n",
      "Epoch: 190 PCA Loss: 4.06681\n",
      "Epoch: 195 PCA Loss: 3.0579946\n",
      "Epoch: 200 PCA Loss: 4.780897\n",
      "Epoch: 205 PCA Loss: 4.89347\n",
      "Epoch: 210 PCA Loss: 4.1528363\n",
      "Epoch: 215 PCA Loss: 7.438438\n",
      "Epoch: 220 PCA Loss: 5.215923\n",
      "Epoch: 225 PCA Loss: 5.3607717\n",
      "Epoch: 230 PCA Loss: 2.5782068\n",
      "Epoch: 235 PCA Loss: 5.8604407\n",
      "Epoch: 240 PCA Loss: 3.4580932\n",
      "Epoch: 245 PCA Loss: 1.8963612\n",
      "Epoch: 250 PCA Loss: 4.63259\n",
      "Epoch: 255 PCA Loss: 1.7702659\n",
      "Epoch: 260 PCA Loss: 1.4816985\n",
      "Epoch: 265 PCA Loss: 0.8338853\n",
      "Epoch: 270 PCA Loss: 1.3327316\n",
      "Epoch: 275 PCA Loss: 0.9479973\n",
      "Epoch: 280 PCA Loss: 34.712322\n",
      "Epoch: 285 PCA Loss: 6.254446\n",
      "Epoch: 290 PCA Loss: 0.815263\n",
      "Epoch: 295 PCA Loss: 0.45413098\n",
      "Epoch: 300 PCA Loss: 0.337973\n",
      "Epoch: 305 PCA Loss: 0.36738077\n",
      "Epoch: 310 PCA Loss: 0.48435736\n",
      "Epoch: 315 PCA Loss: 0.32410935\n",
      "Epoch: 320 PCA Loss: 0.385663\n",
      "Epoch: 325 PCA Loss: 0.44504872\n",
      "Epoch: 330 PCA Loss: 4.607359\n",
      "Epoch: 335 PCA Loss: 5081086.0\n",
      "Epoch: 340 PCA Loss: 57065.734\n",
      "Epoch: 345 PCA Loss: 44242.957\n",
      "Epoch: 350 PCA Loss: 26566.475\n",
      "Epoch: 355 PCA Loss: 14181.329\n",
      "Epoch: 360 PCA Loss: 10420.443\n",
      "Epoch: 365 PCA Loss: 13151.321\n",
      "Epoch: 370 PCA Loss: 10269.152\n",
      "Epoch: 375 PCA Loss: 7224.2793\n",
      "Epoch: 380 PCA Loss: 4921.0825\n",
      "Epoch: 385 PCA Loss: 7020.4146\n",
      "Epoch: 390 PCA Loss: 5632.3755\n",
      "Epoch: 395 PCA Loss: 3622.5225\n",
      "Epoch: 400 PCA Loss: 3994.4678\n",
      "Epoch: 405 PCA Loss: 3390.2822\n",
      "Epoch: 410 PCA Loss: 2897.2466\n",
      "Epoch: 415 PCA Loss: 2425.229\n",
      "Epoch: 420 PCA Loss: 2109.6458\n",
      "Epoch: 425 PCA Loss: 2018.9017\n",
      "Epoch: 430 PCA Loss: 1902.8114\n",
      "Epoch: 435 PCA Loss: 2516.9382\n",
      "Epoch: 440 PCA Loss: 1445.6605\n",
      "Epoch: 445 PCA Loss: 1700.4442\n",
      "Epoch: 450 PCA Loss: 1635.0306\n",
      "Epoch: 455 PCA Loss: 1044.2241\n",
      "Epoch: 460 PCA Loss: 1041.1315\n",
      "Epoch: 465 PCA Loss: 1202.4407\n",
      "Epoch: 470 PCA Loss: 1050.6366\n",
      "Epoch: 475 PCA Loss: 953.9344\n",
      "Epoch: 480 PCA Loss: 872.94293\n",
      "Epoch: 485 PCA Loss: 902.3324\n",
      "Epoch: 490 PCA Loss: 757.6772\n",
      "Epoch: 495 PCA Loss: 640.6358\n",
      "Did 10000 of loss minimized training in 56.51303458213806 seconds.\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "steps_between_test_save = 5\n",
    "batch = 10\n",
    "all_data_steps = 20\n",
    "worst_loss = 12000\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_total.restore(sess, set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 1\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "            idx = np.random.randint(full_train.shape[0], size=batch)\n",
    "            batch_train = full_train[idx,:]\n",
    "            batch_train_label = full_train_labels[idx]\n",
    "            pca_train = np.reshape(np.ravel(batch_train), (batch_train.shape[0] * batch_train.shape[1], 128,128))\n",
    "            \n",
    "            #run Training Op\n",
    "            sess.run([training_op_pca], feed_dict={single_data: pca_train})\n",
    "\n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0) :\n",
    "            idx = np.random.randint(full_test.shape[0], size=batch)\n",
    "            batch_test = full_test[idx,:]\n",
    "            batch_test_label = full_test_labels[idx]\n",
    "            pca_test = np.reshape(np.ravel(batch_test), (batch_test.shape[0] * batch_test.shape[1], 128,128))\n",
    "            \n",
    "            l, l_sum = sess.run([pca_loss, pca_loss_summary], \n",
    "                                                   feed_dict = {single_data: pca_test, training: False, batch_size: batch})\n",
    "\n",
    "            filewriter.add_summary(l_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" PCA Loss: \" + str(l))\n",
    "            if l < worst_loss:\n",
    "                saver_total.save(sess, set_net_model_best)\n",
    "                worst_loss = l\n",
    "            saver_total.save(sess, set_net_model)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    end_time = time.time()\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(step * all_data_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Unique Classifier\n",
    "\n",
    "Here we train the classifeir to distinguish the unique from real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/set_project_brain_code_pca/model/set_project_best\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/set_project_brain_code_pca/logs/set_project_brain_code_pca-run-20190422003209/\n",
      "Epoch: 1 Unique Loss: 0.7571067 Accuracy: 0.2\n",
      "Epoch: 2 Unique Loss: 0.7122079 Accuracy: 0.4\n",
      "Epoch: 3 Unique Loss: 0.71933293 Accuracy: 0.4\n",
      "Epoch: 4 Unique Loss: 0.7362977 Accuracy: 0.33333334\n",
      "Epoch: 5 Unique Loss: 0.6857277 Accuracy: 0.6\n",
      "Epoch: 6 Unique Loss: 0.6450497 Accuracy: 0.6\n",
      "Epoch: 7 Unique Loss: 0.7483855 Accuracy: 0.46666667\n",
      "Epoch: 8 Unique Loss: 0.71125066 Accuracy: 0.46666667\n",
      "Epoch: 9 Unique Loss: 0.69747734 Accuracy: 0.46666667\n",
      "Epoch: 10 Unique Loss: 0.6571738 Accuracy: 0.6666667\n",
      "Epoch: 11 Unique Loss: 0.7010724 Accuracy: 0.46666667\n",
      "Epoch: 12 Unique Loss: 0.5799655 Accuracy: 0.6666667\n",
      "Epoch: 13 Unique Loss: 1.2641436 Accuracy: 0.53333336\n",
      "Epoch: 14 Unique Loss: 0.5201204 Accuracy: 0.73333335\n",
      "Epoch: 15 Unique Loss: 1.1208012 Accuracy: 0.6\n",
      "Epoch: 16 Unique Loss: 1.4184449 Accuracy: 0.4\n",
      "Epoch: 17 Unique Loss: 0.6378004 Accuracy: 0.73333335\n",
      "Epoch: 18 Unique Loss: 1.6763974 Accuracy: 0.26666668\n",
      "Epoch: 19 Unique Loss: 1.8422874 Accuracy: 0.46666667\n",
      "Epoch: 20 Unique Loss: 1.3262141 Accuracy: 0.6666667\n",
      "Epoch: 21 Unique Loss: 0.8388904 Accuracy: 0.6666667\n",
      "Epoch: 22 Unique Loss: 1.0095643 Accuracy: 0.46666667\n",
      "Epoch: 23 Unique Loss: 0.85145986 Accuracy: 0.6\n",
      "Epoch: 24 Unique Loss: 1.7041554 Accuracy: 0.4\n",
      "Epoch: 25 Unique Loss: 1.4229163 Accuracy: 0.46666667\n",
      "Epoch: 26 Unique Loss: 0.5508807 Accuracy: 0.73333335\n",
      "Epoch: 27 Unique Loss: 1.1955783 Accuracy: 0.6\n",
      "Epoch: 28 Unique Loss: 1.7531592 Accuracy: 0.4\n",
      "Epoch: 29 Unique Loss: 2.7820303 Accuracy: 0.2\n",
      "Epoch: 30 Unique Loss: 1.630038 Accuracy: 0.4\n",
      "Epoch: 31 Unique Loss: 0.74193364 Accuracy: 0.6666667\n",
      "Epoch: 32 Unique Loss: 0.91380596 Accuracy: 0.6\n",
      "Epoch: 33 Unique Loss: 0.8783783 Accuracy: 0.53333336\n",
      "Epoch: 34 Unique Loss: 1.2528712 Accuracy: 0.46666667\n",
      "Epoch: 35 Unique Loss: 0.85199326 Accuracy: 0.53333336\n",
      "Epoch: 36 Unique Loss: 1.0841577 Accuracy: 0.53333336\n",
      "Epoch: 37 Unique Loss: 2.3530304 Accuracy: 0.33333334\n",
      "Epoch: 38 Unique Loss: 2.757102 Accuracy: 0.2\n",
      "Epoch: 39 Unique Loss: 0.9080392 Accuracy: 0.6666667\n",
      "Epoch: 40 Unique Loss: 1.7027681 Accuracy: 0.53333336\n",
      "Epoch: 41 Unique Loss: 1.4715734 Accuracy: 0.46666667\n",
      "Epoch: 42 Unique Loss: 1.5560381 Accuracy: 0.53333336\n",
      "Epoch: 43 Unique Loss: 1.6606977 Accuracy: 0.33333334\n",
      "Epoch: 44 Unique Loss: 0.9319698 Accuracy: 0.6\n",
      "Epoch: 45 Unique Loss: 1.8830955 Accuracy: 0.4\n",
      "Epoch: 46 Unique Loss: 1.3586048 Accuracy: 0.4\n",
      "Epoch: 47 Unique Loss: 0.70632833 Accuracy: 0.73333335\n",
      "Epoch: 48 Unique Loss: 1.7268131 Accuracy: 0.33333334\n",
      "Epoch: 49 Unique Loss: 1.0848409 Accuracy: 0.6666667\n",
      "Epoch: 50 Unique Loss: 1.5613327 Accuracy: 0.53333336\n",
      "Epoch: 51 Unique Loss: 1.38001 Accuracy: 0.53333336\n",
      "Epoch: 52 Unique Loss: 1.1629924 Accuracy: 0.46666667\n",
      "Epoch: 53 Unique Loss: 1.5853435 Accuracy: 0.46666667\n",
      "Epoch: 54 Unique Loss: 0.9625709 Accuracy: 0.6\n",
      "Epoch: 55 Unique Loss: 0.90303093 Accuracy: 0.73333335\n",
      "Epoch: 56 Unique Loss: 1.459976 Accuracy: 0.46666667\n",
      "Epoch: 57 Unique Loss: 1.4279097 Accuracy: 0.4\n",
      "Epoch: 58 Unique Loss: 1.8720443 Accuracy: 0.4\n",
      "Epoch: 59 Unique Loss: 1.9021171 Accuracy: 0.46666667\n",
      "Epoch: 60 Unique Loss: 1.2201418 Accuracy: 0.53333336\n",
      "Epoch: 61 Unique Loss: 0.7526488 Accuracy: 0.73333335\n",
      "Epoch: 62 Unique Loss: 1.2917876 Accuracy: 0.6\n",
      "Epoch: 63 Unique Loss: 1.1040382 Accuracy: 0.6666667\n",
      "Epoch: 64 Unique Loss: 1.8241656 Accuracy: 0.4\n",
      "Epoch: 65 Unique Loss: 1.0037836 Accuracy: 0.6666667\n",
      "Epoch: 66 Unique Loss: 1.6167711 Accuracy: 0.46666667\n",
      "Epoch: 67 Unique Loss: 1.1586766 Accuracy: 0.4\n",
      "Epoch: 68 Unique Loss: 1.6197611 Accuracy: 0.53333336\n",
      "Epoch: 69 Unique Loss: 1.0905544 Accuracy: 0.6\n",
      "Epoch: 70 Unique Loss: 1.5615698 Accuracy: 0.53333336\n",
      "Epoch: 71 Unique Loss: 1.6513679 Accuracy: 0.46666667\n",
      "Epoch: 72 Unique Loss: 1.3183053 Accuracy: 0.53333336\n",
      "Epoch: 73 Unique Loss: 1.2971635 Accuracy: 0.33333334\n",
      "Epoch: 74 Unique Loss: 1.4962327 Accuracy: 0.6666667\n",
      "Epoch: 75 Unique Loss: 0.8934378 Accuracy: 0.53333336\n",
      "Epoch: 76 Unique Loss: 1.3881567 Accuracy: 0.33333334\n",
      "Epoch: 77 Unique Loss: 1.4440495 Accuracy: 0.46666667\n",
      "Epoch: 78 Unique Loss: 1.9961842 Accuracy: 0.33333334\n",
      "Epoch: 79 Unique Loss: 2.2880049 Accuracy: 0.46666667\n",
      "Epoch: 80 Unique Loss: 0.85943514 Accuracy: 0.6\n",
      "Epoch: 81 Unique Loss: 2.1363587 Accuracy: 0.46666667\n",
      "Epoch: 82 Unique Loss: 2.2331953 Accuracy: 0.46666667\n",
      "Epoch: 83 Unique Loss: 1.4560809 Accuracy: 0.6666667\n",
      "Epoch: 84 Unique Loss: 1.833987 Accuracy: 0.46666667\n",
      "Epoch: 85 Unique Loss: 1.5092943 Accuracy: 0.33333334\n",
      "Epoch: 86 Unique Loss: 2.4176743 Accuracy: 0.26666668\n",
      "Epoch: 87 Unique Loss: 2.2391636 Accuracy: 0.53333336\n",
      "Epoch: 88 Unique Loss: 1.8201536 Accuracy: 0.53333336\n",
      "Epoch: 89 Unique Loss: 1.6837217 Accuracy: 0.53333336\n",
      "Epoch: 90 Unique Loss: 1.5391691 Accuracy: 0.53333336\n",
      "Epoch: 91 Unique Loss: 2.4328077 Accuracy: 0.4\n",
      "Epoch: 92 Unique Loss: 2.3082297 Accuracy: 0.33333334\n",
      "Epoch: 93 Unique Loss: 2.9546697 Accuracy: 0.46666667\n",
      "Epoch: 94 Unique Loss: 2.0891104 Accuracy: 0.6666667\n",
      "Epoch: 95 Unique Loss: 1.6292608 Accuracy: 0.53333336\n",
      "Epoch: 96 Unique Loss: 1.506976 Accuracy: 0.46666667\n",
      "Epoch: 97 Unique Loss: 1.8717842 Accuracy: 0.53333336\n",
      "Epoch: 98 Unique Loss: 1.6702136 Accuracy: 0.33333334\n",
      "Epoch: 99 Unique Loss: 1.6051723 Accuracy: 0.4\n",
      "Epoch: 100 Unique Loss: 0.75731504 Accuracy: 0.73333335\n",
      "Epoch: 101 Unique Loss: 0.62766016 Accuracy: 0.73333335\n",
      "Epoch: 102 Unique Loss: 2.7449677 Accuracy: 0.53333336\n",
      "Epoch: 103 Unique Loss: 1.6878052 Accuracy: 0.46666667\n",
      "Epoch: 104 Unique Loss: 2.1282349 Accuracy: 0.33333334\n",
      "Epoch: 105 Unique Loss: 1.8922124 Accuracy: 0.4\n",
      "Epoch: 106 Unique Loss: 1.7030952 Accuracy: 0.4\n",
      "Epoch: 107 Unique Loss: 1.3914447 Accuracy: 0.53333336\n",
      "Epoch: 108 Unique Loss: 0.6599074 Accuracy: 0.8\n",
      "Epoch: 109 Unique Loss: 1.5263883 Accuracy: 0.53333336\n",
      "Epoch: 110 Unique Loss: 1.2475266 Accuracy: 0.6\n",
      "Epoch: 111 Unique Loss: 1.5591912 Accuracy: 0.6666667\n",
      "Epoch: 112 Unique Loss: 3.2793014 Accuracy: 0.46666667\n",
      "Epoch: 113 Unique Loss: 1.3833073 Accuracy: 0.53333336\n",
      "Epoch: 114 Unique Loss: 0.8125994 Accuracy: 0.73333335\n",
      "Epoch: 115 Unique Loss: 1.1422646 Accuracy: 0.53333336\n",
      "Epoch: 116 Unique Loss: 3.0145454 Accuracy: 0.4\n",
      "Epoch: 117 Unique Loss: 1.2438253 Accuracy: 0.53333336\n",
      "Epoch: 118 Unique Loss: 2.2949677 Accuracy: 0.6666667\n",
      "Epoch: 119 Unique Loss: 2.574206 Accuracy: 0.46666667\n",
      "Epoch: 120 Unique Loss: 1.9975667 Accuracy: 0.46666667\n",
      "Epoch: 121 Unique Loss: 2.169942 Accuracy: 0.46666667\n",
      "Epoch: 122 Unique Loss: 1.942627 Accuracy: 0.46666667\n",
      "Epoch: 123 Unique Loss: 2.562783 Accuracy: 0.53333336\n",
      "Epoch: 124 Unique Loss: 2.484556 Accuracy: 0.46666667\n",
      "Epoch: 125 Unique Loss: 3.2078629 Accuracy: 0.33333334\n",
      "Epoch: 126 Unique Loss: 1.7304344 Accuracy: 0.53333336\n",
      "Epoch: 127 Unique Loss: 0.9735756 Accuracy: 0.8\n",
      "Epoch: 128 Unique Loss: 1.8087308 Accuracy: 0.6666667\n",
      "Epoch: 129 Unique Loss: 1.7623459 Accuracy: 0.53333336\n",
      "Epoch: 130 Unique Loss: 2.4905777 Accuracy: 0.6\n",
      "Epoch: 131 Unique Loss: 3.1009827 Accuracy: 0.46666667\n",
      "Epoch: 132 Unique Loss: 1.6450403 Accuracy: 0.73333335\n",
      "Epoch: 133 Unique Loss: 3.1671 Accuracy: 0.46666667\n",
      "Epoch: 134 Unique Loss: 2.760862 Accuracy: 0.53333336\n",
      "Epoch: 135 Unique Loss: 2.0903876 Accuracy: 0.53333336\n",
      "Epoch: 136 Unique Loss: 2.2303162 Accuracy: 0.6\n",
      "Epoch: 137 Unique Loss: 1.8022543 Accuracy: 0.46666667\n",
      "Epoch: 138 Unique Loss: 2.1996336 Accuracy: 0.6\n",
      "Epoch: 139 Unique Loss: 1.4236282 Accuracy: 0.6666667\n",
      "Epoch: 140 Unique Loss: 2.404469 Accuracy: 0.46666667\n",
      "Epoch: 141 Unique Loss: 3.8403401 Accuracy: 0.2\n",
      "Epoch: 142 Unique Loss: 2.8014655 Accuracy: 0.33333334\n",
      "Epoch: 143 Unique Loss: 1.3289772 Accuracy: 0.53333336\n",
      "Epoch: 144 Unique Loss: 1.0908921 Accuracy: 0.53333336\n",
      "Epoch: 145 Unique Loss: 0.8976021 Accuracy: 0.8\n",
      "Epoch: 146 Unique Loss: 1.8196921 Accuracy: 0.53333336\n",
      "Epoch: 147 Unique Loss: 1.9511706 Accuracy: 0.53333336\n",
      "Epoch: 148 Unique Loss: 3.5875714 Accuracy: 0.33333334\n",
      "Epoch: 149 Unique Loss: 2.422519 Accuracy: 0.6666667\n",
      "Epoch: 150 Unique Loss: 1.3837523 Accuracy: 0.6666667\n",
      "Epoch: 151 Unique Loss: 2.9814353 Accuracy: 0.33333334\n",
      "Epoch: 152 Unique Loss: 2.1737895 Accuracy: 0.6666667\n",
      "Epoch: 153 Unique Loss: 3.6833842 Accuracy: 0.33333334\n",
      "Epoch: 154 Unique Loss: 2.349186 Accuracy: 0.6\n",
      "Epoch: 155 Unique Loss: 2.0978267 Accuracy: 0.46666667\n",
      "Epoch: 156 Unique Loss: 2.3601227 Accuracy: 0.53333336\n",
      "Epoch: 157 Unique Loss: 4.286281 Accuracy: 0.26666668\n",
      "Epoch: 158 Unique Loss: 3.6379802 Accuracy: 0.33333334\n",
      "Epoch: 159 Unique Loss: 1.7727549 Accuracy: 0.53333336\n",
      "Epoch: 160 Unique Loss: 2.9048808 Accuracy: 0.6\n",
      "Epoch: 161 Unique Loss: 2.3230338 Accuracy: 0.6666667\n",
      "Epoch: 162 Unique Loss: 3.5469449 Accuracy: 0.46666667\n",
      "Epoch: 163 Unique Loss: 1.336467 Accuracy: 0.73333335\n",
      "Epoch: 164 Unique Loss: 3.2024052 Accuracy: 0.46666667\n",
      "Epoch: 165 Unique Loss: 1.1251338 Accuracy: 0.73333335\n",
      "Epoch: 166 Unique Loss: 3.7690046 Accuracy: 0.33333334\n",
      "Epoch: 167 Unique Loss: 2.0009253 Accuracy: 0.6666667\n",
      "Epoch: 168 Unique Loss: 3.7407587 Accuracy: 0.4\n",
      "Epoch: 169 Unique Loss: 3.1441453 Accuracy: 0.46666667\n",
      "Epoch: 170 Unique Loss: 4.988032 Accuracy: 0.4\n",
      "Epoch: 171 Unique Loss: 3.8120248 Accuracy: 0.46666667\n",
      "Epoch: 172 Unique Loss: 2.0490553 Accuracy: 0.6666667\n",
      "Epoch: 173 Unique Loss: 5.3805747 Accuracy: 0.46666667\n",
      "Epoch: 174 Unique Loss: 2.1230757 Accuracy: 0.46666667\n",
      "Epoch: 175 Unique Loss: 2.9427836 Accuracy: 0.46666667\n",
      "Epoch: 176 Unique Loss: 2.1986022 Accuracy: 0.46666667\n",
      "Epoch: 177 Unique Loss: 3.694084 Accuracy: 0.46666667\n",
      "Epoch: 178 Unique Loss: 1.5102489 Accuracy: 0.6\n",
      "Epoch: 179 Unique Loss: 2.0270703 Accuracy: 0.53333336\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-778a9346bd95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0msaver_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_net_model_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mworst_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0msaver_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_net_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1439\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[0;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "steps_between_test_save = 1\n",
    "batch = 15\n",
    "train_size = 600\n",
    "#all_data_steps = np.int(np.floor(train_size/batch))\n",
    "all_data_steps = 40\n",
    "worst_acc = 0\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_total.restore(sess, set_net_model_best)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 1\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < epochs:\n",
    "        for i in range(all_data_steps):\n",
    "            idx = np.random.randint(full_train.shape[0], size=batch)\n",
    "            batch_train = full_train[idx,:]\n",
    "            batch_train_label = full_train_labels[idx]\n",
    "            \n",
    "            #run Training Op\n",
    "            sess.run([training_op_unq], feed_dict={data_set: batch_train, data_label: batch_train_label})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0) :\n",
    "            idx = np.random.randint(full_test.shape[0], size=batch)\n",
    "            batch_test = full_test[idx,:]\n",
    "            batch_test_label = full_test_labels[idx]\n",
    "            \n",
    "            loss_un, loss_un_val, acc, acc_sum = sess.run([loss_summary_unq, loss_unq, accuracy, accuracy_summary], \n",
    "                                                   feed_dict = {data_set: batch_test , data_label: batch_test_label, training: False, batch_size: batch})\n",
    "\n",
    "            filewriter.add_summary(loss_un, step)\n",
    "            filewriter.add_summary(acc_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \" Unique Loss: \" + str(loss_un_val) + \" Accuracy: \" + str(acc))\n",
    "            if acc > worst_acc:\n",
    "                saver_total.save(sess, set_net_model_best)\n",
    "                worst_acc = acc\n",
    "            saver_total.save(sess, set_net_model)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver_total.save(sess, set_net_model)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step_unique)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Training Metrics\n",
    "\n",
    "Here we will get final accuracies for both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/set_project_brain_code_pca/model/set_project\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/set_project_brain_code_pca/logs/set_project_brain_code_pca-run-20190422003209/\n",
      "Final Accuracy: 0.5093749982770532\n"
     ]
    }
   ],
   "source": [
    "batch = 20\n",
    "test_size = full_test.shape[0]\n",
    "all_data_steps = np.int(np.floor(test_size/batch))\n",
    "unique_accuracy = 0\n",
    "start_index = 0\n",
    "end_index = batch\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver_total.restore(sess, set_net_model)\n",
    "    \n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    for i in range(all_data_steps):\n",
    "        idx = np.arange(start_index, end_index)\n",
    "        batch_test = full_test[idx,:]\n",
    "        batch_test_label = full_test_labels[idx]\n",
    "        unq_acc = sess.run(accuracy, feed_dict = {data_set: batch_test , data_label: batch_test_label, training: False, batch_size: batch})\n",
    "        unique_accuracy = ((i  * unique_accuracy) + unq_acc)/(i + 1)\n",
    "        start_index = start_index + batch\n",
    "        end_index = end_index + batch\n",
    "end_time = time.time()\n",
    "print(\"Final Accuracy: \" + str(unique_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5093749982770532"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
